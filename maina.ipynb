{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python377jvsc74a57bd0dcad97eb31a40c02fdb1764e8f108885f6c3fa2d5ede0b775bbb0ea4b2ba36fa",
   "display_name": "Python 3.7.7 64-bit ('myenv': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Much of the structure for this code, more specifically the data pre-processing, was taken from https://www.kaggle.com/looc60/birdsong-recognition-model-input-comparison/data#Introduction\n",
    "\n",
    "# We then built upon the data pre-processing and implemented our own version of the convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/tesfashenkute/opt/miniconda3/envs/myenv/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3072: DtypeWarning: Columns (1,2,3,4,5,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,26,27,28,29,30,31,32,33,34) have mixed types.Specify dtype option on import or set low_memory=False.\n  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# more imports (csv data)\n",
    "train_csv = pd.read_csv(\"/Users/tesfashenkute/Downloads/birdsong-recognition/train.csv\")\n",
    "test_csv = pd.read_csv(\"/Users/tesfashenkute/Downloads/birdsong-recognition/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of      site                                      row_id  seconds  \\\n",
       "0  site_1   site_1_0a997dff022e3ad9744d4e7bbf923288_5        5   \n",
       "1  site_1  site_1_0a997dff022e3ad9744d4e7bbf923288_10       10   \n",
       "2  site_1  site_1_0a997dff022e3ad9744d4e7bbf923288_15       15   \n",
       "\n",
       "                           audio_id  \n",
       "0  0a997dff022e3ad9744d4e7bbf923288  \n",
       "1  0a997dff022e3ad9744d4e7bbf923288  \n",
       "2  0a997dff022e3ad9744d4e7bbf923288  >"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# checking the data\n",
    "train_csv.head\n",
    "test_csv.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Narrowing down bird species to save on computational power\n",
    "# Narrowing down to aldfly, dowwoo and hamfly\n",
    "\n",
    "train_data = pd.DataFrame()\n",
    "\n",
    "for row in range(train_csv.shape[0]) :\n",
    "    for name in ['aldfly','dowwoo', 'hamfly'] :\n",
    "        if train_csv.iloc[row]['ebird_code'] == name :\n",
    "            train_data = train_data.append(train_csv.iloc[row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building audio data path\n",
    "main_dir = '/Users/tesfashenkute/Downloads/birdsong-recognition/train_audio'\n",
    "train_data['full_path'] = main_dir + '/' + train_csv['ebird_code'] + '/' + train_data['filename']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'full_path'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/opt/miniconda3/envs/myenv/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2894\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2895\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'full_path'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-3074e818e071>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# setting audio data according to bird species code (SAMPLE)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0maldfly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ebird_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"aldfly\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m33\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'full_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdowwoo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ebird_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"dowwoo\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m33\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'full_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mhamfly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ebird_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"hamfly\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m33\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'full_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/myenv/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2904\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/myenv/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2895\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'full_path'"
     ]
    }
   ],
   "source": [
    "# setting audio data according to bird species code (SAMPLE)\n",
    "aldfly = train_data[train_data['ebird_code'] == \"aldfly\"].sample(1, random_state = 33)['full_path'].values[0]\n",
    "dowwoo = train_data[train_data['ebird_code'] == \"dowwoo\"].sample(1, random_state = 33)['full_path'].values[0]\n",
    "hamfly = train_data[train_data['ebird_code'] == \"hamfly\"].sample(1, random_state = 33)['full_path'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'aldfly' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-1529d0698548>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maldfly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdowwoo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhamfly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'aldfly' is not defined"
     ]
    }
   ],
   "source": [
    "print(aldfly)\n",
    "print(dowwoo)\n",
    "print(hamfly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking audio samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "aldfly_audio_sample = librosa.load(aldfly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dowwoo_audio_sample = librosa.load(dowwoo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamfly_audio_sample = librosa.load(hamfly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Processing: hamfly\n",
      "processing file 1 on 100, folder 1 on 3\n",
      "processing file 2 on 100, folder 1 on 3\n",
      "processing file 3 on 100, folder 1 on 3\n",
      "processing file 4 on 100, folder 1 on 3\n",
      "processing file 5 on 100, folder 1 on 3\n",
      "processing file 6 on 100, folder 1 on 3\n",
      "processing file 7 on 100, folder 1 on 3\n",
      "processing file 8 on 100, folder 1 on 3\n",
      "processing file 9 on 100, folder 1 on 3\n",
      "processing file 10 on 100, folder 1 on 3\n",
      "processing file 11 on 100, folder 1 on 3\n",
      "processing file 12 on 100, folder 1 on 3\n",
      "processing file 13 on 100, folder 1 on 3\n",
      "processing file 14 on 100, folder 1 on 3\n",
      "processing file 15 on 100, folder 1 on 3\n",
      "processing file 16 on 100, folder 1 on 3\n",
      "processing file 17 on 100, folder 1 on 3\n",
      "processing file 18 on 100, folder 1 on 3\n",
      "processing file 19 on 100, folder 1 on 3\n",
      "processing file 20 on 100, folder 1 on 3\n",
      "processing file 21 on 100, folder 1 on 3\n",
      "processing file 22 on 100, folder 1 on 3\n",
      "processing file 23 on 100, folder 1 on 3\n",
      "processing file 24 on 100, folder 1 on 3\n",
      "processing file 25 on 100, folder 1 on 3\n",
      "processing file 26 on 100, folder 1 on 3\n",
      "processing file 27 on 100, folder 1 on 3\n",
      "processing file 28 on 100, folder 1 on 3\n",
      "processing file 29 on 100, folder 1 on 3\n",
      "processing file 30 on 100, folder 1 on 3\n",
      "processing file 31 on 100, folder 1 on 3\n",
      "processing file 32 on 100, folder 1 on 3\n",
      "processing file 33 on 100, folder 1 on 3\n",
      "processing file 34 on 100, folder 1 on 3\n",
      "processing file 35 on 100, folder 1 on 3\n",
      "processing file 36 on 100, folder 1 on 3\n",
      "processing file 37 on 100, folder 1 on 3\n",
      "processing file 38 on 100, folder 1 on 3\n",
      "processing file 39 on 100, folder 1 on 3\n",
      "processing file 40 on 100, folder 1 on 3\n",
      "processing file 41 on 100, folder 1 on 3\n",
      "processing file 42 on 100, folder 1 on 3\n",
      "processing file 43 on 100, folder 1 on 3\n",
      "processing file 44 on 100, folder 1 on 3\n",
      "processing file 45 on 100, folder 1 on 3\n",
      "processing file 46 on 100, folder 1 on 3\n",
      "processing file 47 on 100, folder 1 on 3\n",
      "processing file 48 on 100, folder 1 on 3\n",
      "processing file 49 on 100, folder 1 on 3\n",
      "processing file 50 on 100, folder 1 on 3\n",
      "processing file 51 on 100, folder 1 on 3\n",
      "processing file 52 on 100, folder 1 on 3\n",
      "processing file 53 on 100, folder 1 on 3\n",
      "processing file 54 on 100, folder 1 on 3\n",
      "processing file 55 on 100, folder 1 on 3\n",
      "processing file 56 on 100, folder 1 on 3\n",
      "processing file 57 on 100, folder 1 on 3\n",
      "processing file 58 on 100, folder 1 on 3\n",
      "processing file 59 on 100, folder 1 on 3\n",
      "processing file 60 on 100, folder 1 on 3\n",
      "processing file 61 on 100, folder 1 on 3\n",
      "processing file 62 on 100, folder 1 on 3\n",
      "processing file 63 on 100, folder 1 on 3\n",
      "processing file 64 on 100, folder 1 on 3\n",
      "processing file 65 on 100, folder 1 on 3\n",
      "processing file 66 on 100, folder 1 on 3\n",
      "processing file 67 on 100, folder 1 on 3\n",
      "processing file 68 on 100, folder 1 on 3\n",
      "processing file 69 on 100, folder 1 on 3\n",
      "processing file 70 on 100, folder 1 on 3\n",
      "processing file 71 on 100, folder 1 on 3\n",
      "processing file 72 on 100, folder 1 on 3\n",
      "processing file 73 on 100, folder 1 on 3\n",
      "processing file 74 on 100, folder 1 on 3\n",
      "processing file 75 on 100, folder 1 on 3\n",
      "processing file 76 on 100, folder 1 on 3\n",
      "processing file 77 on 100, folder 1 on 3\n",
      "processing file 78 on 100, folder 1 on 3\n",
      "processing file 79 on 100, folder 1 on 3\n",
      "processing file 80 on 100, folder 1 on 3\n",
      "processing file 81 on 100, folder 1 on 3\n",
      "processing file 82 on 100, folder 1 on 3\n",
      "processing file 83 on 100, folder 1 on 3\n",
      "processing file 84 on 100, folder 1 on 3\n",
      "processing file 85 on 100, folder 1 on 3\n",
      "processing file 86 on 100, folder 1 on 3\n",
      "processing file 87 on 100, folder 1 on 3\n",
      "processing file 88 on 100, folder 1 on 3\n",
      "processing file 89 on 100, folder 1 on 3\n",
      "processing file 90 on 100, folder 1 on 3\n",
      "processing file 91 on 100, folder 1 on 3\n",
      "processing file 92 on 100, folder 1 on 3\n",
      "processing file 93 on 100, folder 1 on 3\n",
      "processing file 94 on 100, folder 1 on 3\n",
      "processing file 95 on 100, folder 1 on 3\n",
      "processing file 96 on 100, folder 1 on 3\n",
      "processing file 97 on 100, folder 1 on 3\n",
      "processing file 98 on 100, folder 1 on 3\n",
      "processing file 99 on 100, folder 1 on 3\n",
      "processing file 100 on 100, folder 1 on 3\n",
      "\n",
      "Processing: dowwoo\n",
      "processing file 1 on 100, folder 2 on 3\n",
      "processing file 2 on 100, folder 2 on 3\n",
      "processing file 3 on 100, folder 2 on 3\n",
      "processing file 4 on 100, folder 2 on 3\n",
      "processing file 5 on 100, folder 2 on 3\n",
      "processing file 6 on 100, folder 2 on 3\n",
      "processing file 7 on 100, folder 2 on 3\n",
      "processing file 8 on 100, folder 2 on 3\n",
      "processing file 9 on 100, folder 2 on 3\n",
      "processing file 10 on 100, folder 2 on 3\n",
      "processing file 11 on 100, folder 2 on 3\n",
      "processing file 12 on 100, folder 2 on 3\n",
      "processing file 13 on 100, folder 2 on 3\n",
      "processing file 14 on 100, folder 2 on 3\n",
      "processing file 15 on 100, folder 2 on 3\n",
      "processing file 16 on 100, folder 2 on 3\n",
      "processing file 17 on 100, folder 2 on 3\n",
      "processing file 18 on 100, folder 2 on 3\n",
      "processing file 19 on 100, folder 2 on 3\n",
      "processing file 20 on 100, folder 2 on 3\n",
      "processing file 21 on 100, folder 2 on 3\n",
      "processing file 22 on 100, folder 2 on 3\n",
      "processing file 23 on 100, folder 2 on 3\n",
      "processing file 24 on 100, folder 2 on 3\n",
      "processing file 25 on 100, folder 2 on 3\n",
      "processing file 26 on 100, folder 2 on 3\n",
      "processing file 27 on 100, folder 2 on 3\n",
      "processing file 28 on 100, folder 2 on 3\n",
      "processing file 29 on 100, folder 2 on 3\n",
      "processing file 30 on 100, folder 2 on 3\n",
      "processing file 31 on 100, folder 2 on 3\n",
      "processing file 32 on 100, folder 2 on 3\n",
      "processing file 33 on 100, folder 2 on 3\n",
      "processing file 34 on 100, folder 2 on 3\n",
      "processing file 35 on 100, folder 2 on 3\n",
      "processing file 36 on 100, folder 2 on 3\n",
      "processing file 37 on 100, folder 2 on 3\n",
      "processing file 38 on 100, folder 2 on 3\n",
      "processing file 39 on 100, folder 2 on 3\n",
      "processing file 40 on 100, folder 2 on 3\n",
      "processing file 41 on 100, folder 2 on 3\n",
      "processing file 42 on 100, folder 2 on 3\n",
      "processing file 43 on 100, folder 2 on 3\n",
      "processing file 44 on 100, folder 2 on 3\n",
      "processing file 45 on 100, folder 2 on 3\n",
      "processing file 46 on 100, folder 2 on 3\n",
      "processing file 47 on 100, folder 2 on 3\n",
      "processing file 48 on 100, folder 2 on 3\n",
      "processing file 49 on 100, folder 2 on 3\n",
      "processing file 50 on 100, folder 2 on 3\n",
      "processing file 51 on 100, folder 2 on 3\n",
      "processing file 52 on 100, folder 2 on 3\n",
      "processing file 53 on 100, folder 2 on 3\n",
      "processing file 54 on 100, folder 2 on 3\n",
      "processing file 55 on 100, folder 2 on 3\n",
      "processing file 56 on 100, folder 2 on 3\n",
      "processing file 57 on 100, folder 2 on 3\n",
      "processing file 58 on 100, folder 2 on 3\n",
      "processing file 59 on 100, folder 2 on 3\n",
      "processing file 60 on 100, folder 2 on 3\n",
      "processing file 61 on 100, folder 2 on 3\n",
      "processing file 62 on 100, folder 2 on 3\n",
      "processing file 63 on 100, folder 2 on 3\n",
      "processing file 64 on 100, folder 2 on 3\n",
      "processing file 65 on 100, folder 2 on 3\n",
      "processing file 66 on 100, folder 2 on 3\n",
      "processing file 67 on 100, folder 2 on 3\n",
      "processing file 68 on 100, folder 2 on 3\n",
      "processing file 69 on 100, folder 2 on 3\n",
      "processing file 70 on 100, folder 2 on 3\n",
      "processing file 71 on 100, folder 2 on 3\n",
      "processing file 72 on 100, folder 2 on 3\n",
      "processing file 73 on 100, folder 2 on 3\n",
      "processing file 74 on 100, folder 2 on 3\n",
      "processing file 75 on 100, folder 2 on 3\n",
      "processing file 76 on 100, folder 2 on 3\n",
      "processing file 77 on 100, folder 2 on 3\n",
      "processing file 78 on 100, folder 2 on 3\n",
      "processing file 79 on 100, folder 2 on 3\n",
      "processing file 80 on 100, folder 2 on 3\n",
      "processing file 81 on 100, folder 2 on 3\n",
      "processing file 82 on 100, folder 2 on 3\n",
      "processing file 83 on 100, folder 2 on 3\n",
      "processing file 84 on 100, folder 2 on 3\n",
      "processing file 85 on 100, folder 2 on 3\n",
      "processing file 86 on 100, folder 2 on 3\n",
      "processing file 87 on 100, folder 2 on 3\n",
      "processing file 88 on 100, folder 2 on 3\n",
      "processing file 89 on 100, folder 2 on 3\n",
      "processing file 90 on 100, folder 2 on 3\n",
      "processing file 91 on 100, folder 2 on 3\n",
      "processing file 92 on 100, folder 2 on 3\n",
      "processing file 93 on 100, folder 2 on 3\n",
      "processing file 94 on 100, folder 2 on 3\n",
      "processing file 95 on 100, folder 2 on 3\n",
      "processing file 96 on 100, folder 2 on 3\n",
      "processing file 97 on 100, folder 2 on 3\n",
      "processing file 98 on 100, folder 2 on 3\n",
      "processing file 99 on 100, folder 2 on 3\n",
      "processing file 100 on 100, folder 2 on 3\n",
      "\n",
      "Processing: aldfly\n",
      "processing file 1 on 100, folder 3 on 3\n",
      "processing file 2 on 100, folder 3 on 3\n",
      "processing file 3 on 100, folder 3 on 3\n",
      "processing file 4 on 100, folder 3 on 3\n",
      "processing file 5 on 100, folder 3 on 3\n",
      "processing file 6 on 100, folder 3 on 3\n",
      "processing file 7 on 100, folder 3 on 3\n",
      "processing file 8 on 100, folder 3 on 3\n",
      "processing file 9 on 100, folder 3 on 3\n",
      "processing file 10 on 100, folder 3 on 3\n",
      "processing file 11 on 100, folder 3 on 3\n",
      "processing file 12 on 100, folder 3 on 3\n",
      "processing file 13 on 100, folder 3 on 3\n",
      "processing file 14 on 100, folder 3 on 3\n",
      "processing file 15 on 100, folder 3 on 3\n",
      "processing file 16 on 100, folder 3 on 3\n",
      "processing file 17 on 100, folder 3 on 3\n",
      "processing file 18 on 100, folder 3 on 3\n",
      "processing file 19 on 100, folder 3 on 3\n",
      "processing file 20 on 100, folder 3 on 3\n",
      "processing file 21 on 100, folder 3 on 3\n",
      "processing file 22 on 100, folder 3 on 3\n",
      "processing file 23 on 100, folder 3 on 3\n",
      "processing file 24 on 100, folder 3 on 3\n",
      "processing file 25 on 100, folder 3 on 3\n",
      "processing file 26 on 100, folder 3 on 3\n",
      "processing file 27 on 100, folder 3 on 3\n",
      "processing file 28 on 100, folder 3 on 3\n",
      "processing file 29 on 100, folder 3 on 3\n",
      "processing file 30 on 100, folder 3 on 3\n",
      "processing file 31 on 100, folder 3 on 3\n",
      "processing file 32 on 100, folder 3 on 3\n",
      "processing file 33 on 100, folder 3 on 3\n",
      "processing file 34 on 100, folder 3 on 3\n",
      "processing file 35 on 100, folder 3 on 3\n",
      "processing file 36 on 100, folder 3 on 3\n",
      "processing file 37 on 100, folder 3 on 3\n",
      "processing file 38 on 100, folder 3 on 3\n",
      "processing file 39 on 100, folder 3 on 3\n",
      "processing file 40 on 100, folder 3 on 3\n",
      "processing file 41 on 100, folder 3 on 3\n",
      "processing file 42 on 100, folder 3 on 3\n",
      "processing file 43 on 100, folder 3 on 3\n",
      "processing file 44 on 100, folder 3 on 3\n",
      "processing file 45 on 100, folder 3 on 3\n",
      "processing file 46 on 100, folder 3 on 3\n",
      "processing file 47 on 100, folder 3 on 3\n",
      "processing file 48 on 100, folder 3 on 3\n",
      "processing file 49 on 100, folder 3 on 3\n",
      "processing file 50 on 100, folder 3 on 3\n",
      "processing file 51 on 100, folder 3 on 3\n",
      "processing file 52 on 100, folder 3 on 3\n",
      "processing file 53 on 100, folder 3 on 3\n",
      "processing file 54 on 100, folder 3 on 3\n",
      "processing file 55 on 100, folder 3 on 3\n",
      "processing file 56 on 100, folder 3 on 3\n",
      "processing file 57 on 100, folder 3 on 3\n",
      "processing file 58 on 100, folder 3 on 3\n",
      "processing file 59 on 100, folder 3 on 3\n",
      "processing file 60 on 100, folder 3 on 3\n",
      "processing file 61 on 100, folder 3 on 3\n",
      "processing file 62 on 100, folder 3 on 3\n",
      "processing file 63 on 100, folder 3 on 3\n",
      "processing file 64 on 100, folder 3 on 3\n",
      "processing file 65 on 100, folder 3 on 3\n",
      "processing file 66 on 100, folder 3 on 3\n",
      "processing file 67 on 100, folder 3 on 3\n",
      "processing file 68 on 100, folder 3 on 3\n",
      "processing file 69 on 100, folder 3 on 3\n",
      "processing file 70 on 100, folder 3 on 3\n",
      "processing file 71 on 100, folder 3 on 3\n",
      "processing file 72 on 100, folder 3 on 3\n",
      "processing file 73 on 100, folder 3 on 3\n",
      "processing file 74 on 100, folder 3 on 3\n",
      "processing file 75 on 100, folder 3 on 3\n",
      "processing file 76 on 100, folder 3 on 3\n",
      "processing file 77 on 100, folder 3 on 3\n",
      "processing file 78 on 100, folder 3 on 3\n",
      "processing file 79 on 100, folder 3 on 3\n",
      "processing file 80 on 100, folder 3 on 3\n",
      "processing file 81 on 100, folder 3 on 3\n",
      "processing file 82 on 100, folder 3 on 3\n",
      "processing file 83 on 100, folder 3 on 3\n",
      "processing file 84 on 100, folder 3 on 3\n",
      "processing file 85 on 100, folder 3 on 3\n",
      "processing file 86 on 100, folder 3 on 3\n",
      "processing file 87 on 100, folder 3 on 3\n",
      "processing file 88 on 100, folder 3 on 3\n",
      "processing file 89 on 100, folder 3 on 3\n",
      "processing file 90 on 100, folder 3 on 3\n",
      "processing file 91 on 100, folder 3 on 3\n",
      "processing file 92 on 100, folder 3 on 3\n",
      "processing file 93 on 100, folder 3 on 3\n",
      "processing file 94 on 100, folder 3 on 3\n",
      "processing file 95 on 100, folder 3 on 3\n",
      "processing file 96 on 100, folder 3 on 3\n",
      "processing file 97 on 100, folder 3 on 3\n",
      "processing file 98 on 100, folder 3 on 3\n",
      "processing file 99 on 100, folder 3 on 3\n",
      "processing file 100 on 100, folder 3 on 3\n",
      "Data successfully saved !\n"
     ]
    }
   ],
   "source": [
    "# Generating STFT\n",
    "\n",
    "import json \n",
    "import os\n",
    "import math\n",
    "import librosa\n",
    "import warnings\n",
    "import numpy as np\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "DATASET_PATH = \"/Users/tesfashenkute/Downloads/birdsong-recognition/train_audio/\"\n",
    "JSON_PATH = \"STFT.json\"\n",
    "SAMPLE_RATE = 22050\n",
    "\n",
    "TO_PROCESS = [\"aldfly\", \"dowwoo\",\"hamfly\"] # Species we want to process (decreases computational time im on a macbook pro lol)\n",
    "\n",
    "# parameters changes because STFTs are much more memory-consuming than MFCCs\n",
    "def save_stft(dataset_path, json_path, n_fft=512, hop_length=2048, segment_duration=4): \n",
    "    \"\"\"Extracts STFTs from music dataset and saves them into a json file along witgh genre labels.\n",
    "\n",
    "        :param dataset_path (str): Path to dataset\n",
    "        :param json_path (str): Path to json file used to save STFTs\n",
    "        :param n_fft (int): Interval we consider to apply FFT. Measured in # of samples\n",
    "        :param hop_length (int): Sliding window for FFT. Measured in # of samples\n",
    "        :param: num_segments (int): Number of segments we want to divide sample tracks into\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "    # dictionary to store mapping, labels, and STFTs\n",
    "    data = {\n",
    "        \"mapping\": [], # genres\n",
    "        \"labels\": [], # a number (corresponding to a genres) : targets that we expect\n",
    "        \"stft\": [] # will bbe the inputs\n",
    "    }\n",
    "    \n",
    "    file_count = 0 # keeps teack of the loading process\n",
    "\n",
    "    # loop through all genre sub-folder\n",
    "    for i, (dirpath, dirnames, filenames) in enumerate(os.walk(dataset_path)):\n",
    "        \n",
    "        # ensure we're processing a genre sub-folder level\n",
    "        if dirpath is not dataset_path:\n",
    "\n",
    "            # save genre label (i.e., sub-folder name) in the mapping\n",
    "            semantic_label = dirpath.split('/')[-1]\n",
    "            \n",
    "            # Proceed to data extraction only for few species\n",
    "            if semantic_label in TO_PROCESS:\n",
    "                \n",
    "                # Keeps track of loading process\n",
    "                file_count = file_count + 1\n",
    "            \n",
    "                data[\"mapping\"].append(semantic_label)\n",
    "                print(\"\\nProcessing: {}\".format(semantic_label))\n",
    "\n",
    "                # process all audio files in genre sub-dir\n",
    "                \n",
    "                num_file = 0\n",
    "                for f in filenames:\n",
    "                    num_file += 1\n",
    "                    \n",
    "                    # audio file\n",
    "                    file_path = os.path.join(dirpath, f)\n",
    "                    signal, sample_rate = librosa.load(file_path, sr=SAMPLE_RATE) # audio file in array\n",
    "\n",
    "                    audio_duration = librosa.get_duration(signal, sr=SAMPLE_RATE) # different duration for each sample\n",
    "                    num_segments = int(audio_duration // segment_duration) # number of segments the audio can be cut into\n",
    "                    # we want audios of the same duration to allow comparisons\n",
    "\n",
    "                    samples_per_segment = int(SAMPLE_RATE * segment_duration)\n",
    "                    num_stft_vectors_per_segment = math.ceil(samples_per_segment / hop_length)\n",
    "                    # that will be what we study\n",
    "\n",
    "                    \n",
    "                    # print(\"{}, segment:{}\".format(file_path, d+1), end = '\\r', flush=True)\n",
    "                    print(\"processing file {} on {}, folder {} on {}\".format(num_file,len(filenames),file_count,len(TO_PROCESS)))\n",
    "                    \n",
    "                    # process all segments of audio file\n",
    "                    for d in range(num_segments):\n",
    "\n",
    "                        # calculate start and finish sample for current segment\n",
    "                        start = samples_per_segment * d  # sample at which the segment begin\n",
    "                        finish = start + samples_per_segment # sample at which the segment stops\n",
    "\n",
    "                        # extract stft (what we will use)\n",
    "                        stft = np.abs(librosa.stft(signal[start:finish], \n",
    "                                                    n_fft=n_fft, \n",
    "                                                    hop_length=hop_length))\n",
    "                        stft = librosa.amplitude_to_db(stft, ref = np.max)\n",
    "                        stft = stft.T\n",
    "\n",
    "\n",
    "                        # store only stft feature with expected number of vectors\n",
    "                        if len(stft) == num_stft_vectors_per_segment: \n",
    "                            data[\"stft\"].append(stft.tolist())\n",
    "                            data[\"labels\"].append(i-1)\n",
    "\n",
    "\n",
    "    # save STFTs to json file\n",
    "    with open(json_path, \"w\") as fp:\n",
    "        json.dump(data, fp, indent = 4)\n",
    "        print(\"Data successfully saved !\")  \n",
    "\n",
    "# Function execution\n",
    "save_stft(DATASET_PATH, JSON_PATH, segment_duration=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Processing: hamfly\n",
      "\n",
      "Processing: dowwoo\n",
      "\n",
      "Processing: aldfly\n",
      "Data successfully saved !\n"
     ]
    }
   ],
   "source": [
    "# Generating MFCC\n",
    "\n",
    "import json \n",
    "import os\n",
    "import math\n",
    "import librosa\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "DATASET_PATH = \"/Users/tesfashenkute/Downloads/birdsong-recognition/train_audio/\"\n",
    "JSON_PATH = \"MFCC.json\"\n",
    "SAMPLE_RATE = 22050\n",
    "\n",
    "TO_PROCESS = [\"aldfly\", \"dowwoo\",\"hamfly\"] # Species we want to process (decreases computational time im on a macbook pro lol)\n",
    "\n",
    "# main function\n",
    "def save_mfcc(dataset_path, json_path, num_mfcc=13, n_fft=2048, hop_length=512, segment_duration=6):\n",
    "    \"\"\"Extracts MFCCs from music dataset and saves them into a json file along witgh genre labels.\n",
    "\n",
    "        :param dataset_path (str): Path to dataset\n",
    "        :param json_path (str): Path to json file used to save MFCCs\n",
    "        :param num_mfcc (int): Number of coefficients to extract\n",
    "        :param n_fft (int): Interval we consider to apply FFT. Measured in # of samples\n",
    "        :param hop_length (int): Sliding window for FFT. Measured in # of samples\n",
    "        :param: num_segments (int): Number of segments we want to divide sample tracks into\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "    # dictionary to store mapping, labels, and MFCCs\n",
    "    data = {\n",
    "        \"mapping\": [], # genres\n",
    "        \"labels\": [], # a number (corresponding to a genres) : targets that we expect\n",
    "        \"mfcc\": [] # will bbe the inputs\n",
    "    }\n",
    "    \n",
    "\n",
    "    # loop through all genre sub-folder\n",
    "    for i, (dirpath, dirnames, filenames) in enumerate(os.walk(dataset_path)):\n",
    "\n",
    "        # ensure we're processing a genre sub-folder level\n",
    "        if dirpath is not dataset_path:\n",
    "\n",
    "            # save genre label (i.e., sub-folder name) in the mapping\n",
    "            semantic_label = dirpath.split('/')[-1]\n",
    "            \n",
    "            # Proceed to data extraction only for few species\n",
    "            if semantic_label in TO_PROCESS:\n",
    "            \n",
    "                data[\"mapping\"].append(semantic_label)\n",
    "                print(\"\\nProcessing: {}\".format(semantic_label))\n",
    "\n",
    "                # process all audio files in genre sub-dir\n",
    "                \n",
    "                num_file = 0\n",
    "                for f in filenames:\n",
    "                    num_file += 1\n",
    "                    \n",
    "                    # audio file\n",
    "                    file_path = os.path.join(dirpath, f)\n",
    "                    signal, sample_rate = librosa.load(file_path, sr=SAMPLE_RATE) # audio file in array\n",
    "\n",
    "                    audio_duration = librosa.get_duration(signal, sr=SAMPLE_RATE) # different duration for each sample\n",
    "                    num_segments = int(audio_duration // segment_duration) # number of segments the audio can be cut into\n",
    "                    # we want audios of the same duration to allow comparisons\n",
    "\n",
    "                    samples_per_segment = int(SAMPLE_RATE * segment_duration)\n",
    "                    num_mfcc_vectors_per_segment = math.ceil(samples_per_segment / hop_length)\n",
    "                    # that will be what we study\n",
    "\n",
    "                    # process all segments of audio file\n",
    "                    for d in range(num_segments):\n",
    "\n",
    "                        # calculate start and finish sample for current segment\n",
    "                        start = samples_per_segment * d  # sample at which the segment begin\n",
    "                        finish = start + samples_per_segment # sample at which the segment stops\n",
    "\n",
    "                        # extract mfcc (what we will use)\n",
    "                        mfcc = librosa.feature.mfcc(signal[start:finish], \n",
    "                                                    sample_rate, \n",
    "                                                    n_mfcc=num_mfcc, \n",
    "                                                    n_fft=n_fft, \n",
    "                                                    hop_length=hop_length)\n",
    "                        mfcc = mfcc.T\n",
    "\n",
    "\n",
    "                        # store only mfcc feature with expected number of vectors\n",
    "                        if len(mfcc) == num_mfcc_vectors_per_segment: \n",
    "                            data[\"mfcc\"].append(mfcc.tolist())\n",
    "                            data[\"labels\"].append(i-1)\n",
    "                            # print(\"{}, segment:{}\".format(file_path, d+1), end = '\\r', flush=True)\n",
    "                            print(\"processing file {} on {}\".format(num_file,len(filenames)), end = '\\r', flush=True)\n",
    "\n",
    "    # save MFCCs to json file\n",
    "    with open(json_path, \"w\") as fp:\n",
    "        json.dump(data, fp, indent = 4)\n",
    "        print(\"Data successfully saved !\")       \n",
    "\n",
    "# Function execution\n",
    "save_mfcc(DATASET_PATH, JSON_PATH, segment_duration=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import tensorflow.keras as keras\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH_MFCC = \"MFCC.json\"\n",
    "DATA_PATH_STFT = \"STFT.json\"\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data set from json file\n",
    "\n",
    "def load_data(data_path,data_type):\n",
    "    with open(data_path, \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    X = np.array(data[data_type])\n",
    "    y = np.array(data[\"labels\"])\n",
    "\n",
    "    print(\"Data loaded.\")\n",
    "    \n",
    "    # Resetting indexes\n",
    "    values = list(set(y))\n",
    "    \n",
    "    for i in range(len(values)):\n",
    "        y[y == values[i]] = i\n",
    "        \n",
    "    # returns the inputs to be split in the following function\n",
    "    return  X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(dataset, test_size, validation_size):\n",
    "    if (dataset == \"MFCC\"):\n",
    "        # loading MFCC data to inputs using load_data\n",
    "        X, y = load_data(DATA_PATH_MFCC,\"mfcc\")\n",
    "    \n",
    "    if (dataset == \"STFT\"):\n",
    "        # loading STFT data to inputs using load_data\n",
    "        X, y = load_data(DATA_PATH_STFT,\"stft\")    \n",
    "\n",
    "    # splitting test, training and validation data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "    X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=validation_size)\n",
    "\n",
    "    # returning test, training and validation split\n",
    "    return X_train, X_validation, X_test, y_train, y_validation, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_axis(X_train, X_validation, X_test):\n",
    "    # building inputs from new axis\n",
    "\n",
    "    X_train = X_train[..., np.newaxis]\n",
    "    X_validation = X_validation[..., np.newaxis]\n",
    "    X_test = X_test[..., np.newaxis]\n",
    "    \n",
    "    return X_train, X_validation, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Building final dataset..\n",
      "Data loaded.\n",
      "Data loaded.\n",
      "Dataset built successfully\n"
     ]
    }
   ],
   "source": [
    "# Building final data\n",
    "\n",
    "print(\"Building final dataset..\")\n",
    "\n",
    "# we will be using MFCC data as the reference and STFT to compare\n",
    "X_train, X_validation, X_test, y_train, y_validation, y_test = prepare_datasets(\"MFCC\", 0.25, 0.2)\n",
    "X2_train, X2_validation, X2_test, y2_train, y2_validation, y2_test = prepare_datasets(\"STFT\", 0.25, 0.2)\n",
    "\n",
    "print(\"Dataset built successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Neural Network\n",
    "# Will attempt using input layer, 5 convolutional layers (all with relu activation function) and an output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will build the CNN model for our training\n",
    "def cnn_build(input_shape, output_shape):\n",
    "    \n",
    "    # initializing network\n",
    "    cnn_model = keras.Sequential()\n",
    "\n",
    "    # Layer 1\n",
    "    cnn_model.add(keras.layers.Conv2D(64, (4, 4), activation='relu', input_shape=input_shape))\n",
    "    cnn_model.add(keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same'))\n",
    "    cnn_model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "    # Layer 2\n",
    "    cnn_model.add(keras.layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "    cnn_model.add(keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same'))\n",
    "    cnn_model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "    # Layer 3\n",
    "    cnn_model.add(keras.layers.Conv2D(64, (2, 2), activation='relu', input_shape=input_shape))\n",
    "    cnn_model.add(keras.layers.MaxPooling2D((2, 2), strides=(2, 2), padding='same'))\n",
    "    cnn_model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "    # Inputting into output layer (flattening -> dense)\n",
    "    cnn_model.add(keras.layers.Flatten())\n",
    "    cnn_model.add(keras.layers.Dense(64, activation='relu'))\n",
    "    cnn_model.add(keras.layers.Dropout(0.5))\n",
    "\n",
    "    # Output\n",
    "    cnn_model.add(keras.layers.Dense(output_shape, activation='softmax'))\n",
    "\n",
    "    return cnn_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting output parameter and labels\n",
    "\n",
    "birds = [\"aldfly\", \"dowwoo\",\"hamfly\"]\n",
    "output_shape = len(birds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add new axis to inputs for CNN ###\n",
    "X_train_new, X_validation_new, X_test_new = add_axis(X_train, X_validation, X_test)\n",
    "X2_train_new, X2_validation_new, X2_test_new = add_axis(X2_train, X2_validation, X2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "\tCONVOLUTIONAL MODEL (MFCCs)\n",
      "\n",
      "Creating network...\n",
      "Network created !\n",
      "\n",
      "Compiling Network...\n",
      "Network compiled !\n",
      "\n",
      "Convolutional model summary :\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 256, 10, 64)       1088      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 128, 5, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 128, 5, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 126, 3, 32)        18464     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 63, 2, 32)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 63, 2, 32)         128       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 62, 1, 64)         8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 31, 1, 64)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 31, 1, 64)         256       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1984)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                127040    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 155,683\n",
      "Trainable params: 155,363\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n",
      "\n",
      "Training model...\n",
      "Epoch 1/30\n",
      "52/52 [==============================] - 5s 89ms/step - loss: 1.4817 - accuracy: 0.4253 - val_loss: 1.0487 - val_accuracy: 0.4495\n",
      "Epoch 2/30\n",
      "52/52 [==============================] - 5s 89ms/step - loss: 0.9409 - accuracy: 0.5817 - val_loss: 0.8824 - val_accuracy: 0.5385\n",
      "Epoch 3/30\n",
      "52/52 [==============================] - 4s 82ms/step - loss: 0.8427 - accuracy: 0.6170 - val_loss: 0.7028 - val_accuracy: 0.6587\n",
      "Epoch 4/30\n",
      "52/52 [==============================] - 4s 83ms/step - loss: 0.7168 - accuracy: 0.6887 - val_loss: 0.6035 - val_accuracy: 0.7380\n",
      "Epoch 5/30\n",
      "52/52 [==============================] - 4s 81ms/step - loss: 0.5970 - accuracy: 0.7478 - val_loss: 0.5491 - val_accuracy: 0.7668\n",
      "Epoch 6/30\n",
      "52/52 [==============================] - 4s 82ms/step - loss: 0.5508 - accuracy: 0.7771 - val_loss: 0.4906 - val_accuracy: 0.8221\n",
      "Epoch 7/30\n",
      "52/52 [==============================] - 4s 82ms/step - loss: 0.5088 - accuracy: 0.7629 - val_loss: 0.4445 - val_accuracy: 0.8438\n",
      "Epoch 8/30\n",
      "52/52 [==============================] - 4s 82ms/step - loss: 0.4354 - accuracy: 0.8215 - val_loss: 0.4254 - val_accuracy: 0.8510\n",
      "Epoch 9/30\n",
      "52/52 [==============================] - 4s 82ms/step - loss: 0.4424 - accuracy: 0.8356 - val_loss: 0.4059 - val_accuracy: 0.8534\n",
      "Epoch 10/30\n",
      "52/52 [==============================] - 4s 82ms/step - loss: 0.4331 - accuracy: 0.8131 - val_loss: 0.3880 - val_accuracy: 0.8654\n",
      "Epoch 11/30\n",
      "52/52 [==============================] - 4s 83ms/step - loss: 0.3752 - accuracy: 0.8538 - val_loss: 0.3716 - val_accuracy: 0.8702\n",
      "Epoch 12/30\n",
      "52/52 [==============================] - 4s 82ms/step - loss: 0.3170 - accuracy: 0.8835 - val_loss: 0.3726 - val_accuracy: 0.8726\n",
      "Epoch 13/30\n",
      "52/52 [==============================] - 4s 82ms/step - loss: 0.2796 - accuracy: 0.9009 - val_loss: 0.3322 - val_accuracy: 0.8894\n",
      "Epoch 14/30\n",
      "52/52 [==============================] - 4s 82ms/step - loss: 0.2894 - accuracy: 0.8946 - val_loss: 0.3166 - val_accuracy: 0.9087\n",
      "Epoch 15/30\n",
      "52/52 [==============================] - 4s 83ms/step - loss: 0.2795 - accuracy: 0.8950 - val_loss: 0.3262 - val_accuracy: 0.8822\n",
      "Epoch 16/30\n",
      "52/52 [==============================] - 5s 90ms/step - loss: 0.2572 - accuracy: 0.8986 - val_loss: 0.3146 - val_accuracy: 0.9038\n",
      "Epoch 17/30\n",
      "52/52 [==============================] - 5s 93ms/step - loss: 0.2422 - accuracy: 0.9126 - val_loss: 0.3093 - val_accuracy: 0.9183\n",
      "Epoch 18/30\n",
      "52/52 [==============================] - 5s 92ms/step - loss: 0.2226 - accuracy: 0.9211 - val_loss: 0.3158 - val_accuracy: 0.9087\n",
      "Epoch 19/30\n",
      "52/52 [==============================] - 5s 99ms/step - loss: 0.2142 - accuracy: 0.9296 - val_loss: 0.2783 - val_accuracy: 0.9159\n",
      "Epoch 20/30\n",
      "52/52 [==============================] - 5s 93ms/step - loss: 0.1942 - accuracy: 0.9256 - val_loss: 0.2848 - val_accuracy: 0.9183\n",
      "Epoch 21/30\n",
      "52/52 [==============================] - 4s 85ms/step - loss: 0.2177 - accuracy: 0.9094 - val_loss: 0.3087 - val_accuracy: 0.8990\n",
      "Epoch 22/30\n",
      "52/52 [==============================] - 4s 77ms/step - loss: 0.1806 - accuracy: 0.9345 - val_loss: 0.2806 - val_accuracy: 0.9135\n",
      "Epoch 23/30\n",
      "52/52 [==============================] - 4s 83ms/step - loss: 0.1822 - accuracy: 0.9373 - val_loss: 0.2694 - val_accuracy: 0.9255\n",
      "Epoch 24/30\n",
      "52/52 [==============================] - 4s 84ms/step - loss: 0.1861 - accuracy: 0.9316 - val_loss: 0.2834 - val_accuracy: 0.9135\n",
      "Epoch 25/30\n",
      "52/52 [==============================] - 4s 84ms/step - loss: 0.1744 - accuracy: 0.9359 - val_loss: 0.2487 - val_accuracy: 0.9375\n",
      "Epoch 26/30\n",
      "52/52 [==============================] - 4s 84ms/step - loss: 0.1534 - accuracy: 0.9453 - val_loss: 0.2451 - val_accuracy: 0.9375\n",
      "Epoch 27/30\n",
      "52/52 [==============================] - 4s 85ms/step - loss: 0.1556 - accuracy: 0.9446 - val_loss: 0.2473 - val_accuracy: 0.9303\n",
      "Epoch 28/30\n",
      "52/52 [==============================] - 4s 84ms/step - loss: 0.1296 - accuracy: 0.9609 - val_loss: 0.2448 - val_accuracy: 0.9375\n",
      "Epoch 29/30\n",
      "52/52 [==============================] - 4s 84ms/step - loss: 0.1372 - accuracy: 0.9573 - val_loss: 0.2989 - val_accuracy: 0.9135\n",
      "Epoch 30/30\n",
      "52/52 [==============================] - 4s 84ms/step - loss: 0.1829 - accuracy: 0.9247 - val_loss: 0.2692 - val_accuracy: 0.9159\n",
      "Model is trained !\n",
      "\n",
      "Evaluating model...\n",
      "22/22 - 1s - loss: 0.2754 - accuracy: 0.8932\n",
      "Test accuracy : 0.893217921257019\n",
      "\n",
      "Computational time of convolutional NN : 854.8230189999999 seconds\n"
     ]
    }
   ],
   "source": [
    "### CONVOLUTIONAL MODEL (MFCCs) ###\n",
    "print(\"\\n\\n\\tCONVOLUTIONAL MODEL (MFCCs)\\n\")\n",
    "\n",
    "time_start = time.process_time()\n",
    "\n",
    "    # Store shape of the input data (3-tuple)\n",
    "input_shape = (X_train_new.shape[1], X_train_new.shape[2], 1)\n",
    "\n",
    "    # create network\n",
    "print(\"Creating network...\")\n",
    "model_conv = cnn_build(input_shape, output_shape)\n",
    "print(\"Network created !\\n\")\n",
    "\n",
    "    # compile model\n",
    "print(\"Compiling Network...\")\n",
    "optimiser = keras.optimizers.Adam(learning_rate=0.0001) # Adam optimizer\n",
    "model_conv.compile(optimizer=optimiser,\n",
    "                    loss='sparse_categorical_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "print(\"Network compiled !\")\n",
    "\n",
    "print(\"\\nConvolutional model summary :\")\n",
    "model_conv.summary()\n",
    "\n",
    "    # train model\n",
    "print(\"\\nTraining model...\")\n",
    "history_conv = model_conv.fit(X_train_new, y_train, validation_data=(X_validation_new, y_validation), batch_size=32, epochs=30)\n",
    "print(\"Model is trained !\\n\")\n",
    "\n",
    "    # evaluate model on test set\n",
    "print(\"Evaluating model...\")\n",
    "test_loss_conv, test_acc_conv = model_conv.evaluate(X_test_new, y_test, verbose=2)\n",
    "print('Test accuracy :', test_acc_conv)\n",
    "\n",
    "    # Calculate computational time\n",
    "time_conv = (time.process_time() - time_start)\n",
    "print(\"\\nComputational time of convolutional NN :\", time_conv, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}